{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient()\n",
    "client = MongoClient('localhost', 27017)\n",
    "# client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client.db_cdiscount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import bson                       # this is installed with the pymongo package\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.data import imread   # or, whatever image library you prefer\n",
    "import multiprocessing as mp      # will come in handy due to the size of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "import keras\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first = 0\n",
    "num_batch = 4\n",
    "# last = 82\n",
    "last = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "num_classes = db.cat_encode.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_model(epochs=epochs, num_classes=num_classes, lrate=0.01, momentum=0.9):\n",
    "    \n",
    "    # Convolutional Layer\n",
    "    model = Sequential()\n",
    "    # Convolutional Layer\n",
    "    model.add(Conv2D(40, (3,3), input_shape = (180,180,3), activation='relu'))\n",
    "\n",
    "    # Pooling Layer\n",
    "    model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "    \n",
    "    # Fully conected Layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "    lrate = 0.01\n",
    "    decay = lrate/epochs\n",
    "    sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model =simple_model(epochs=epochs, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# counter = 0\n",
    "\n",
    "# category_id = db.train_example.find({}, {'category_id':1, \"_id\" : 0})\n",
    "# category_id = [i['category_id'] for i in list(category_id)]\n",
    "\n",
    "# cat = db.cat_encode.find({\"cat\" : category_id},{\"cat\" : 1.0, \"_id\" : 0})\n",
    "# cat = list(cat)\n",
    "\n",
    "# # category_id.pop\n",
    "# print(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tmppic1=np.array([])\n",
    "# tmppic2=np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "counter = 0\n",
    "for j in range(0,num_batch-1):\n",
    "# for j in range(0,1):\n",
    "    \n",
    "    a1 = int(list(np.linspace(first,last,num_batch))[j])\n",
    "    an = int(list(np.linspace(first,last,num_batch))[j+1])\n",
    "    \n",
    "#     print(a1,an)\n",
    "    \n",
    "    cur = db.train.find({})[a1:an]\n",
    "    \n",
    "    m= an-a1\n",
    "    \n",
    "    X_batch = np.zeros((m,180,180,3))  \n",
    "    Y_batch = np.zeros((m,num_classes), dtype=int)\n",
    "\n",
    "#     X_batch = np.empty((180,180,3))\n",
    "#     Y_batch = np.empty((num_classes))\n",
    "    \n",
    "#     print('X-Batch shape: ', X_batch.shape )\n",
    "#     print('Y-Batch shape: ', Y_batch.shape )\n",
    "    \n",
    "    i = 0\n",
    "#     pic_array\n",
    "    while (cur.alive):\n",
    "        idx = cur.next()\n",
    "        category_id =idx['category_id'] \n",
    "    #     print(category_id)\n",
    "\n",
    "        cat = db.cat_encode.find_one({ \"cat\" : (category_id)}, {\"cat\" : 1.0, \"_id\" : 0})['cat']\n",
    "    #     print(cat)\n",
    "        \n",
    "#         picture = np.float32(imread(io.BytesIO(idx['imgs'][0]['picture']))/255.0)\n",
    "        picture = np.float32(imread(io.BytesIO(idx['imgs'][0]['picture']))/255.0)\n",
    "        picture = picture.reshape(180,180,3)\n",
    "        \n",
    "        X_batch[i,:,:,:] = picture\n",
    "        \n",
    "        \n",
    "        encode = db.cat_encode.find_one({ \"cat\" : (category_id)}, {\"encode\" : 1.0, \"_id\" : 0})['encode']\n",
    "    #     print(encode)\n",
    "        Y_batch[i,:] = np.array(encode)[0]\n",
    "\n",
    "        \n",
    "        i=i + 1\n",
    "    \n",
    "#     print(Y_batch)\n",
    "    print('encode shape', Y_batch.shape)\n",
    "    print('Pic shape', X_batch.shape)\n",
    "    \n",
    "#     Y_batch1 = Y_batch\n",
    "#     X_batch1 = X_batch\n",
    "    \n",
    "    model.fit(X_batch, Y_batch, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "    del(X_batch)\n",
    "    del(Y_batch)\n",
    "    \n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# del(cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lst_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "counter = 0\n",
    "for j in range(0,num_batch-1):\n",
    "# for j in range(0,1):\n",
    "    \n",
    "    a1 = int(list(np.linspace(first,last,num_batch))[j])\n",
    "    an = int(list(np.linspace(first,last,num_batch))[j+1])\n",
    "    \n",
    "    lst_batch = []\n",
    "    ## training Data\n",
    "    for i in (db.train.find({})[a1:an]):\n",
    "        dic = {}\n",
    "    #     print('category_id: ', i['category_id'])\n",
    "        dic['category_id'] = i['category_id']\n",
    "\n",
    "    #     print('cat: ', db.cat_encode.find_one({ \"cat\" : (1000004079)}, {\"cat\" : 1.0, \"_id\" : 0})['cat'])\n",
    "        dic['cat'] = db.cat_encode.find_one({ \"cat\" : (i['category_id'])}, {\"cat\" : 1.0, \"_id\" : 0})['cat']\n",
    "\n",
    "    #     print('img: ', i['imgs'])\n",
    "#         dic['imgs'] = i['imgs']\n",
    "        picture = imread(io.BytesIO(i['imgs'][0]['picture']))\n",
    "    ## divide image by 255.0\n",
    "        picture = picture/255.0\n",
    "        dic['picture'] = picture.reshape(180,180,3)\n",
    "\n",
    "    #     print('encode: ', db.cat_encode.find_one({ \"cat\" : (1000004079)}, {\"encode\" : 1.0, \"_id\" : 0})['encode'])\n",
    "        dic['encode'] = db.cat_encode.find_one({ \"cat\" : (i['category_id'])}, {\"encode\" : 1.0, \"_id\" : 0})['encode']\n",
    "\n",
    "        lst_batch.append(dic)\n",
    "             \n",
    "        counter = counter +1\n",
    "        \n",
    "    \n",
    "    X_batch = np.array([lst['picture'] for lst in lst_batch])\n",
    "    Y_batch = np.array([lst['encode'] for lst in lst_batch])\n",
    "#     print(X_batch.shape)\n",
    "#     print(Y_batch.shape)\n",
    "    \n",
    "#     print(Y_batch)\n",
    "    print(X_batch.shape)\n",
    "    print(Y_batch.shape)\n",
    "    \n",
    "    \n",
    "#     Y_batch2 = Y_batch\n",
    "#     X_batch2 = X_batch\n",
    "\n",
    "    \n",
    "    model.fit(X_batch, Y_batch, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "    #, validation_split=0.2\n",
    "print(an)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_batch1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_batch2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(X_batch1.shape)\n",
    "# print(X_batch2.shape)\n",
    "# # print(np.equal(X_batch1[0], X_batch2[0]))\n",
    "# t = 0\n",
    "# f = 0\n",
    "# for j in (np.equal(X_batch1[0], X_batch2[0])):\n",
    "#     print(j)\n",
    "# #     if(i is True):\n",
    "# #         t = t+1\n",
    "# #     if(i is False):\n",
    "# #         f = f+1\n",
    "        \n",
    "# print(t,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(np.unique([cat['category_id'] for cat in lst_batch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# num_classes=len(dummy_y[81])\n",
    "# epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1666, 180, 180, 3)\n",
      "(1666, 5270)\n",
      "Train on 1332 samples, validate on 334 samples\n",
      "Epoch 1/10\n",
      "1332/1332 [==============================] - 12s - loss: 5.2992 - acc: 0.3994 - val_loss: 4.4043 - val_acc: 0.4311\n",
      "Epoch 2/10\n",
      "1332/1332 [==============================] - 9s - loss: 3.6737 - acc: 0.4257 - val_loss: 4.3837 - val_acc: 0.4311\n",
      "Epoch 3/10\n",
      "1332/1332 [==============================] - 8s - loss: 3.2098 - acc: 0.4377 - val_loss: 4.3746 - val_acc: 0.4491\n",
      "Epoch 4/10\n",
      "1332/1332 [==============================] - 8s - loss: 2.8597 - acc: 0.4527 - val_loss: 4.1703 - val_acc: 0.4611\n",
      "Epoch 5/10\n",
      "1332/1332 [==============================] - 8s - loss: 2.5288 - acc: 0.4797 - val_loss: 4.2854 - val_acc: 0.4581\n",
      "Epoch 6/10\n",
      "1332/1332 [==============================] - 8s - loss: 2.1516 - acc: 0.5105 - val_loss: 4.4207 - val_acc: 0.4671\n",
      "Epoch 7/10\n",
      "1332/1332 [==============================] - 8s - loss: 1.9070 - acc: 0.5480 - val_loss: 4.6171 - val_acc: 0.4641\n",
      "Epoch 8/10\n",
      "1332/1332 [==============================] - 9s - loss: 1.6609 - acc: 0.6119 - val_loss: 4.6469 - val_acc: 0.4760\n",
      "Epoch 9/10\n",
      "1332/1332 [==============================] - 9s - loss: 1.3554 - acc: 0.6674 - val_loss: 4.7000 - val_acc: 0.4760\n",
      "Epoch 10/10\n",
      "1332/1332 [==============================] - 8s - loss: 1.1953 - acc: 0.6907 - val_loss: 4.8106 - val_acc: 0.4820\n",
      "(1667, 180, 180, 3)\n",
      "(1667, 5270)\n",
      "Train on 1333 samples, validate on 334 samples\n",
      "Epoch 1/10\n",
      "1333/1333 [==============================] - 9s - loss: 4.1031 - acc: 0.4831 - val_loss: 4.3319 - val_acc: 0.4701\n",
      "Epoch 2/10\n",
      "1333/1333 [==============================] - 8s - loss: 3.1967 - acc: 0.5079 - val_loss: 4.1589 - val_acc: 0.4641\n",
      "Epoch 3/10\n",
      "1333/1333 [==============================] - 9s - loss: 2.5373 - acc: 0.5349 - val_loss: 4.4699 - val_acc: 0.4820\n",
      "Epoch 4/10\n",
      "1333/1333 [==============================] - 9s - loss: 2.1237 - acc: 0.5874 - val_loss: 4.7008 - val_acc: 0.4910\n",
      "Epoch 5/10\n",
      "1333/1333 [==============================] - 9s - loss: 1.7782 - acc: 0.6362 - val_loss: 4.4663 - val_acc: 0.5030\n",
      "Epoch 6/10\n",
      "1333/1333 [==============================] - 9s - loss: 1.5073 - acc: 0.6902 - val_loss: 4.6989 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1333/1333 [==============================] - 9s - loss: 1.3566 - acc: 0.7269 - val_loss: 4.6397 - val_acc: 0.5120\n",
      "Epoch 8/10\n",
      "1333/1333 [==============================] - 8s - loss: 1.1857 - acc: 0.7592 - val_loss: 4.7360 - val_acc: 0.4850\n",
      "Epoch 9/10\n",
      "1333/1333 [==============================] - 9s - loss: 1.0550 - acc: 0.7937 - val_loss: 4.8361 - val_acc: 0.4850\n",
      "Epoch 10/10\n",
      "1333/1333 [==============================] - 9s - loss: 0.9687 - acc: 0.8177 - val_loss: 5.0352 - val_acc: 0.4970\n",
      "(1667, 180, 180, 3)\n",
      "(1667, 5270)\n",
      "Train on 1333 samples, validate on 334 samples\n",
      "Epoch 1/10\n",
      " 448/1333 [=========>....................] - ETA: 5s - loss: 4.9130 - acc: 0.4799"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7a28e4850a90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--- %s seconds ---\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hamza\\anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    843\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 845\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    846\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mc:\\users\\hamza\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1485\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1487\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hamza\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1140\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1141\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hamza\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2073\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2074\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hamza\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hamza\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hamza\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mc:\\users\\hamza\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hamza\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "counter = 0\n",
    "for j in range(0,num_batch-1):\n",
    "# for j in range(0,1):\n",
    "    \n",
    "    a1 = int(list(np.linspace(first,last,num_batch))[j])\n",
    "    an = int(list(np.linspace(first,last,num_batch))[j+1])\n",
    "    \n",
    "    lst_batch = []\n",
    "    \n",
    "#     print(a1,an)\n",
    "    \n",
    "    cur = db.train.find({})[a1:an]\n",
    "    \n",
    "    m= an-a1\n",
    "        \n",
    "    i = 0\n",
    "#     pic_array\n",
    "    while (cur.alive):\n",
    "        idx = cur.next()\n",
    "        dic = {}\n",
    "        \n",
    "        category_id =idx['category_id'] \n",
    "    #     print(category_id)\n",
    "\n",
    "        cat = db.cat_encode.find_one({ \"cat\" : (category_id)}, {\"cat\" : 1.0, \"_id\" : 0})['cat']\n",
    "    #     print(cat)\n",
    "        \n",
    "        picture = np.float32(imread(io.BytesIO(idx['imgs'][0]['picture']))/255.0)\n",
    "        picture = picture.reshape(180,180,3)\n",
    "        \n",
    "        dic['picture'] = picture\n",
    "        \n",
    "        encode = db.cat_encode.find_one({ \"cat\" : (category_id)}, {\"encode\" : 1.0, \"_id\" : 0})['encode']\n",
    "    #     print(encode)\n",
    "        dic['encode'] = encode\n",
    "        \n",
    "        lst_batch.append(dic)\n",
    "        \n",
    "        i+=1\n",
    "    X_batch = np.array([lst['picture'] for lst in lst_batch])\n",
    "    Y_batch = np.array([lst['encode'] for lst in lst_batch])\n",
    "    print(X_batch.shape)\n",
    "    print(Y_batch.shape)\n",
    "    \n",
    "\n",
    "    \n",
    "    model.fit(X_batch, Y_batch, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
